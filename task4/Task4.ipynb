{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exclusive-revision",
   "metadata": {},
   "source": [
    "\n",
    "![desc](Screenshot%20from%202021-05-19%2012-05-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-roller",
   "metadata": {},
   "source": [
    "\n",
    "![desc2](Screenshot%20from%202021-05-19%2012-05-28.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thorough-essay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.17.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (49.2.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\david\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glob2 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\david\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.5.2.52)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opencv-python) (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\david\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# installing required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install glob2\n",
    "!{sys.executable} -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "south-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, InputLayer, Input, Dropout\n",
    "\n",
    "from tensorflow.keras.applications import vgg16 \n",
    "from tensorflow.keras.applications import vgg19 \n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relevant-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable NVIDIA CUDA/GPU usage\n",
    "# NVIDIA CUDA must be installed beforehand\n",
    "# (this is not a necessary step)\n",
    "\n",
    "gpu = tensorflow.config.list_physical_devices('GPU')[0]\n",
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf7595-1937-4491-87f8-8ca33fc8bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\"\"\"\n",
    "Here, we have two options. The first one is to go through all triplets, combine them into a single image, and then add this new image to a numpy array. \n",
    "In the end you return a complete array containing all new images (three in one combined), and use this array as your X.\n",
    "The problem with this is that it is very memory intensive, since all images are in memory at once as your feature array.\n",
    "Also it is less flexible. You specify the desired image size in the beginning, and then just have them like this.\n",
    "\n",
    "Second option:\n",
    "as before, go through all triplets and combine them into a single image. but instead returning them as numpy arrays, store these new images as actual image files, i.e. jpg, with a complete directory\n",
    "structure etc.\n",
    "Now we can use these new images very conveniently with keras' built in functions to load datasets from images/directories, we can also further resize them etc.\n",
    "Upside: flexible and practical\n",
    "Downside: you need a lot of disk space. for image size 300x400 (approx. the original image size), you would require ~22 GB of space.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde70252-fa3e-4589-b241-e043322e431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# OLD! FIRST OPTION (worse option). go to next cell for better way to create image features\n",
    "# --------------------------------------------------------------------------\n",
    "# DATA PREPARATION\n",
    "\n",
    "# this function loads the three images and combines them into one for all triplet, and then returns them as a numpy array\n",
    "# you can store the created numpy arrays with numpy functions directly to a file for later use, so only execute this function once and then just load from the file\n",
    "\n",
    "\n",
    "# upside: you dont have to store images on disk\n",
    "# downside: less flexible, you have the images as they are. Also, very memory intensive!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    triplets_df - dataframe of triplets\n",
    "    imgset - \"train\" or \"test\"\n",
    "    height, width - what size to resize the images to (for a single image). the final size will be (height, width*3)\n",
    "\"\"\"\n",
    "def create_image_data_old(triplets_df,imgset=\"train\", height=64,width=64):\n",
    "   \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(triplets_df.index.values)):\n",
    "        \n",
    "        # collect paths for the 3 images\n",
    "        img_paths = []\n",
    "        for j in range(3):\n",
    "            img_path = str(triplets_df.iloc[i,j])\n",
    "            for k in range(5-len(img_path)):\n",
    "                img_path = \"0\" + img_path  # pad img_path with leading zeros\n",
    "            img_paths.append(\"food/\" + imgset + \"/\" + img_path + \".jpg\")\n",
    "       \n",
    "        # combine the 3 images into 1 single image\n",
    "        inputImages=[]\n",
    "        outputImage = np.zeros((height,width*3,3), dtype=\"uint8\")\n",
    "        \n",
    "        for imgpath in img_paths:\n",
    "            image = cv2.imread(imgpath)\n",
    "            image = cv2.resize(image, (width,height))\n",
    "            inputImages.append(image)\n",
    "            \n",
    "        # problem: labels\n",
    "        # in all our data, the first image is more similar to the second.\n",
    "        # to get evenly distributed labels, we will just randomly swap the second and third picture (and therefore the label)\n",
    "        \n",
    "        label = np.random.randint(0,2)\n",
    "        # label == 1 means first image is similar to second one, 0 means first similar to third one\n",
    "        \n",
    "        if imgset == \"test\":\n",
    "            label = 1\n",
    "            \n",
    "            \n",
    "        # depending on label, swap second and third image    \n",
    "        outputImage[0:height, 0:width] = inputImages[0]\n",
    "        outputImage[0:height, width:2*width] = inputImages[2-label]\n",
    "        outputImage[0:height, 2*width:3*width] = inputImages[1+label]\n",
    "        \n",
    "        images.append(outputImage)\n",
    "        labels.append(label)\n",
    "\n",
    "        \n",
    "    return np.array(images), np.array(labels)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9f09eb-cdc2-445c-a45d-90a477377fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION & ORGANIZATION\n",
    "# this function is only executed ONCE to create the img data directory structure, and to create and save the images that will be used as features\n",
    "# it goes through all the triplets, combines the 3 images into one and saves them as a jpg.\n",
    "# all necessary folders, also \"target_directory\", are created automatically\n",
    "# train images are directly split into a train and a valid set, according to the fraction \"validation_size\"\n",
    "# parameters \"width\" and \"height\" are used to resize the images. they correspond to the dimensions of a single image, so the final image will have size (height, width*3)! \n",
    "# The original images are around 300 x 400 in size.\n",
    "\n",
    "# this requires quite a lot of disk space (22 GB with width 400 and height 300)!\n",
    "\n",
    "# the original single images are assumed to be stored in subfolders food/train/ and food/test/\n",
    "\n",
    "\"\"\"\n",
    "    train_data - dataframe of train triplets\n",
    "    test_data - dataframe of test triplets\n",
    "    target_directory - directory name, will be created and the new images will be stored inside\n",
    "    width - desired width of a single image\n",
    "    height - desired height of a single image\n",
    "    validation_size - fraction of the train images that will be put aside to use as a validation set\n",
    "\"\"\"\n",
    "def create_image_data(train_data, test_data,target_directory='img',width=64,height=64,validation_size=0.2):\n",
    "\n",
    "    \n",
    "    if os.path.isdir(target_directory):\n",
    "        print(\"target directory already exists.\")\n",
    "        return\n",
    "    \n",
    "    #create directories, for every set split the images into class-subfolders (0 and 1)\n",
    "    os.makedirs(target_directory + '/train/0')\n",
    "    os.makedirs(target_directory + '/train/1')\n",
    "    os.makedirs(target_directory + '/valid/0')\n",
    "    os.makedirs(target_directory + '/valid/1')\n",
    "    os.makedirs(target_directory + '/test/unknown')\n",
    "\n",
    "    # split train images into train and valid set\n",
    "    train, valid = train_test_split(train_data,test_size=validation_size)\n",
    "    data = {\n",
    "        'train': train,\n",
    "        'valid': valid,\n",
    "        'test': test_data\n",
    "    }\n",
    "    for dataset in data:\n",
    "        written = 0\n",
    "        labels = []\n",
    "        for i in range(len(data[dataset].index.values)):\n",
    "            imgs = []\n",
    "            # load the three images and resize\n",
    "            for j in range(3):\n",
    "                img_path = str(data[dataset].iloc[i,j])\n",
    "                for k in range(5-len(img_path)):\n",
    "                    img_path = \"0\" + img_path  # pad img_path with leading zeros\n",
    "                imgs.append(cv2.imread(\"food/\" + ('test' if dataset == 'test' else 'train') + \"/\" + img_path + \".jpg\"))\n",
    "                imgs[j] = cv2.resize(imgs[j], (width,height))\n",
    "\n",
    "            # problem: labels\n",
    "            # in all our data, the first image is more similar to the second.\n",
    "            # to get evenly distributed labels, we will just randomly swap the second and third picture (and therefore the label)\n",
    "\n",
    "            label = np.random.randint(0,2)\n",
    "            \n",
    "            # label == 1 means first image is similar to second one, 0 means first similar to third one\n",
    "            \n",
    "            # this label swapping is only required for train and valid set\n",
    "            if dataset == \"test\":\n",
    "                label = 1\n",
    "\n",
    "\n",
    "            # swap\n",
    "            if label == 0:\n",
    "                temp = imgs[1]\n",
    "                imgs[1] = imgs[2]\n",
    "                imgs[2] = temp\n",
    "\n",
    "\n",
    "\n",
    "            labels.append(label)\n",
    "\n",
    "            # concatenate the 3 images horizontally\n",
    "            out = cv2.hconcat(imgs)\n",
    "            \n",
    "            path = target_directory + '/' + dataset + '/' + ('unknown' if dataset == 'test' else str(label)) + '/' + str(i) + '.jpg'\n",
    "\n",
    "            # save image\n",
    "            if cv2.imwrite(path, out):\n",
    "                written += 1\n",
    "            else:\n",
    "                print('error when writing image')\n",
    "\n",
    "                \n",
    "\n",
    "        if dataset != 'test':\n",
    "            np.save(target_directory+'/' + dataset + '_labels', np.array(labels))\n",
    "        print(\"--------------------------\")\n",
    "        print(str(written) + \" images written to \" + dataset)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d99ad5-728e-414a-823c-ef251875f743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "47612 images written to train\n",
      "--------------------------\n",
      "11903 images written to valid\n",
      "--------------------------\n",
      "59544 images written to test\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE THIS FUNCTION ONLY ONCE TO CREATE THE NEW IMAGE FEATURES (Create and save images)\n",
    "# (second option, creating and storing new image files)\n",
    "width = 400\n",
    "height = 300\n",
    "\n",
    "\n",
    "train_triplets = pd.read_csv('train_triplets.csv',sep=' ',header=None)\n",
    "test_triplets = pd.read_csv('test_triplets.csv',sep=' ',header=None)\n",
    "\n",
    "\n",
    "create_image_data(train_triplets, test_triplets,'img_400x300',width=width,height=height,validation_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caee2e60-3b6c-4e00-b2af-b8f45e8d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "# stored images will be resized to this width/height (per single image, so full image will have size (height, width*3))\n",
    "width = 200\n",
    "height = 150\n",
    "\n",
    "# batch size\n",
    "bs = 10\n",
    "# epochs\n",
    "ep = 50\n",
    "\n",
    "# increase this if you want to remove more layers of the pretrained vision model\n",
    "remove_layers = 1 # default = 1\n",
    "\n",
    "# increase this if you want to set layers of the pretrained vision model as \"trainable\"\n",
    "layers_trainable = 1 # default = 1\n",
    "\n",
    "# activation function\n",
    "dense_layers_activation = 'relu' # default = 'relu'\n",
    "\n",
    "\n",
    "# pretrained model: VGG16, VGG19, ResNet50, InceptionV3, ...\n",
    "pretrained = vgg16.VGG16(weights='imagenet',include_top=False, input_shape=(height,3*width, 3))\n",
    "preprocess = vgg16.preprocess_input\n",
    "decode = vgg16.decode_predictions\n",
    "\n",
    "\n",
    "# optimizer\n",
    "opt = 'adam' # adam, SGD, RMSprop...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "experienced-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 200, 900, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 200, 900, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 200, 900, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 100, 450, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 100, 450, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 100, 450, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 50, 225, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 50, 225, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 50, 225, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 50, 225, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 25, 112, 256)      0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 25, 112, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 25, 112, 512)      2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 25, 112, 512)      2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 12, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 12, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 12, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 12, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 28, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 86016)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              88081408  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 102,797,121\n",
      "Trainable params: 88,082,433\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TRANSFER LEARNING\n",
    "\n",
    "# use a pretrained vision model to extract useful features from the images, then use these features for binary classification with a new model/new layers\n",
    "\n",
    "# question: should add new convolutional layers, or only Dense (fully connected) layers?\n",
    "# i.e. should we try to detect new image features, or just use the ones we have and directly go to binary classification on these features?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = pretrained\n",
    "\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-remove_layers].output)\n",
    "class1 = Dense(1024, activation=dense_layers_activation)(flat1)\n",
    "#class2 = Dense(128, activation=dense_layers_activation)(class1)\n",
    "#dropout = Dropout(0.2)(class1) # dropout helps with overfitting (but we dont have overfitting yet :( )\n",
    "\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(class1) # equivalent or better than using 2 output neurons with softmax activation\n",
    "\n",
    "# set layers as trainable\n",
    "for layer in model.layers[:-layers_trainable]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# create new model\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "    \n",
    "# summarize\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abc7b7f-0e6a-4f5c-96df-2b2a7c1a9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd99535-187c-4949-9ac9-d1c721112f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47612 images belonging to 2 classes.\n",
      "Found 11903 images belonging to 2 classes.\n",
      "Found 59544 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "# assuming new image files have been created\n",
    "\n",
    "# directory where the images have been stored\n",
    "source_dir = 'img_400x300'\n",
    "\n",
    "\n",
    "#y_train = np.load(source_dir + '/train_labels.npy')\n",
    "#y_valid = np.load(source_dir + '/valid_labels.npy')\n",
    "\n",
    "# classes are inferred from the folder structure\n",
    "\n",
    "train_path = source_dir + '/train'\n",
    "valid_path = source_dir + '/valid'\n",
    "test_path = source_dir + '/test'\n",
    "\n",
    "# create data generators\n",
    "train_batches = ImageDataGenerator(preprocessing_function=preprocess).flow_from_directory(directory=train_path, target_size=(height,3*width), classes=None,class_mode='binary',batch_size=bs)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=preprocess).flow_from_directory(directory=valid_path, target_size=(height,3*width), classes=None,class_mode='binary',batch_size=bs)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=preprocess).flow_from_directory(directory=test_path, target_size=(height,3*width), classes=None,class_mode=None, batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b28a-03a4-4143-96a4-bb32b31082b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a batch of images. keep in mind that they are already preprocessed, so the colors look strange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "imgs, labels = next(train_batches)\n",
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beneficial-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4762/4762 [==============================] - 797s 167ms/step - loss: 2.0839 - accuracy: 0.6038 - val_loss: 0.7582 - val_accuracy: 0.5747\n",
      "Epoch 2/50\n",
      "4762/4762 [==============================] - 783s 164ms/step - loss: 0.6327 - accuracy: 0.6591 - val_loss: 0.6903 - val_accuracy: 0.6580\n",
      "Epoch 3/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.5761 - accuracy: 0.7007 - val_loss: 0.6188 - val_accuracy: 0.6690\n",
      "Epoch 4/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.5108 - accuracy: 0.7287 - val_loss: 0.6823 - val_accuracy: 0.6668\n",
      "Epoch 5/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4793 - accuracy: 0.7459 - val_loss: 0.6644 - val_accuracy: 0.6703\n",
      "Epoch 6/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4578 - accuracy: 0.7612 - val_loss: 0.7906 - val_accuracy: 0.6879\n",
      "Epoch 7/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4457 - accuracy: 0.7725 - val_loss: 0.6822 - val_accuracy: 0.6968\n",
      "Epoch 8/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4351 - accuracy: 0.7781 - val_loss: 0.8509 - val_accuracy: 0.6861\n",
      "Epoch 9/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4155 - accuracy: 0.7885 - val_loss: 0.8591 - val_accuracy: 0.6867\n",
      "Epoch 10/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4102 - accuracy: 0.7953 - val_loss: 0.9530 - val_accuracy: 0.6765\n",
      "Epoch 11/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.4038 - accuracy: 0.7966 - val_loss: 1.0432 - val_accuracy: 0.6907\n",
      "Epoch 12/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3908 - accuracy: 0.8036 - val_loss: 1.2326 - val_accuracy: 0.6964\n",
      "Epoch 13/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3949 - accuracy: 0.8036 - val_loss: 1.2812 - val_accuracy: 0.6766\n",
      "Epoch 14/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3894 - accuracy: 0.8076 - val_loss: 0.8886 - val_accuracy: 0.6915\n",
      "Epoch 15/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3880 - accuracy: 0.8099 - val_loss: 0.8395 - val_accuracy: 0.6959\n",
      "Epoch 16/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3776 - accuracy: 0.8139 - val_loss: 1.2030 - val_accuracy: 0.6860\n",
      "Epoch 17/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3743 - accuracy: 0.8165 - val_loss: 1.0754 - val_accuracy: 0.6947\n",
      "Epoch 18/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3754 - accuracy: 0.8171 - val_loss: 1.2925 - val_accuracy: 0.7060\n",
      "Epoch 19/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3720 - accuracy: 0.8199 - val_loss: 1.1865 - val_accuracy: 0.6866\n",
      "Epoch 20/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3740 - accuracy: 0.8170 - val_loss: 1.4532 - val_accuracy: 0.6768\n",
      "Epoch 21/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3812 - accuracy: 0.8122 - val_loss: 1.2953 - val_accuracy: 0.6876\n",
      "Epoch 22/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3788 - accuracy: 0.8141 - val_loss: 1.5166 - val_accuracy: 0.6792\n",
      "Epoch 23/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3740 - accuracy: 0.8179 - val_loss: 1.2709 - val_accuracy: 0.6954\n",
      "Epoch 24/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3702 - accuracy: 0.8204 - val_loss: 1.5857 - val_accuracy: 0.6942\n",
      "Epoch 25/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3752 - accuracy: 0.8197 - val_loss: 1.6517 - val_accuracy: 0.6851\n",
      "Epoch 26/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3689 - accuracy: 0.8231 - val_loss: 1.8280 - val_accuracy: 0.6895\n",
      "Epoch 27/50\n",
      "4762/4762 [==============================] - 783s 164ms/step - loss: 0.3655 - accuracy: 0.8221 - val_loss: 1.6052 - val_accuracy: 0.7043\n",
      "Epoch 28/50\n",
      "4762/4762 [==============================] - 781s 164ms/step - loss: 0.3614 - accuracy: 0.8242 - val_loss: 1.8422 - val_accuracy: 0.6945\n",
      "Epoch 29/50\n",
      "4762/4762 [==============================] - 783s 164ms/step - loss: 0.3704 - accuracy: 0.8235 - val_loss: 1.6336 - val_accuracy: 0.6902\n",
      "Epoch 30/50\n",
      "4762/4762 [==============================] - 777s 163ms/step - loss: 0.3662 - accuracy: 0.8242 - val_loss: 2.0218 - val_accuracy: 0.6998\n",
      "Epoch 31/50\n",
      "4762/4762 [==============================] - 781s 164ms/step - loss: 0.3657 - accuracy: 0.8269 - val_loss: 2.7429 - val_accuracy: 0.6930\n",
      "Epoch 32/50\n",
      "4762/4762 [==============================] - 781s 164ms/step - loss: 0.3655 - accuracy: 0.8251 - val_loss: 3.2405 - val_accuracy: 0.6952\n",
      "Epoch 33/50\n",
      "4762/4762 [==============================] - 783s 164ms/step - loss: 0.3612 - accuracy: 0.8275 - val_loss: 1.9396 - val_accuracy: 0.7027\n",
      "Epoch 34/50\n",
      "4762/4762 [==============================] - 797s 167ms/step - loss: 0.3534 - accuracy: 0.8296 - val_loss: 2.0123 - val_accuracy: 0.6890\n",
      "Epoch 35/50\n",
      "4762/4762 [==============================] - 781s 164ms/step - loss: 0.3601 - accuracy: 0.8282 - val_loss: 2.0424 - val_accuracy: 0.6841\n",
      "Epoch 36/50\n",
      "4762/4762 [==============================] - 779s 163ms/step - loss: 0.3615 - accuracy: 0.8294 - val_loss: 1.8759 - val_accuracy: 0.6914\n",
      "Epoch 37/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3599 - accuracy: 0.8308 - val_loss: 1.9694 - val_accuracy: 0.6910\n",
      "Epoch 38/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3581 - accuracy: 0.8296 - val_loss: 2.3799 - val_accuracy: 0.7007\n",
      "Epoch 39/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3578 - accuracy: 0.8298 - val_loss: 2.0913 - val_accuracy: 0.6936\n",
      "Epoch 40/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3620 - accuracy: 0.8265 - val_loss: 3.0457 - val_accuracy: 0.6830\n",
      "Epoch 41/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3607 - accuracy: 0.8277 - val_loss: 2.0493 - val_accuracy: 0.6823\n",
      "Epoch 42/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3617 - accuracy: 0.8272 - val_loss: 2.6872 - val_accuracy: 0.6901\n",
      "Epoch 43/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3661 - accuracy: 0.8300 - val_loss: 2.6826 - val_accuracy: 0.6856\n",
      "Epoch 44/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3602 - accuracy: 0.8282 - val_loss: 3.3735 - val_accuracy: 0.6900\n",
      "Epoch 45/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3643 - accuracy: 0.8301 - val_loss: 2.1748 - val_accuracy: 0.6697\n",
      "Epoch 46/50\n",
      "4762/4762 [==============================] - 775s 162ms/step - loss: 0.3560 - accuracy: 0.8293 - val_loss: 3.9762 - val_accuracy: 0.6962\n",
      "Epoch 47/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3650 - accuracy: 0.8269 - val_loss: 3.3555 - val_accuracy: 0.6827\n",
      "Epoch 48/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3671 - accuracy: 0.8280 - val_loss: 2.1608 - val_accuracy: 0.6899\n",
      "Epoch 49/50\n",
      "4762/4762 [==============================] - 774s 162ms/step - loss: 0.3688 - accuracy: 0.8288 - val_loss: 2.7885 - val_accuracy: 0.6839\n",
      "Epoch 50/50\n",
      "4762/4762 [==============================] - 776s 163ms/step - loss: 0.3621 - accuracy: 0.8297 - val_loss: 3.7255 - val_accuracy: 0.6918\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(x=train_batches,verbose=1,epochs=ep,steps_per_epoch=len(train_batches),validation_data=valid_batches,validation_steps=len(valid_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "018e05db-f677-47ed-8b36-c11b673c9f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ee0fa8-f1e5-4c45-8c1a-f86c32f8f049",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretty_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce199070454e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpretty_good\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pretty_good' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0968913-cf61-4909-b278-46f55f2c9b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11863b8-d66c-49fa-968b-846e4131e6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38993c47-7444-4716-b49c-3c37a3e15d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "funded-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're not here yet :(\n",
    "y_pred = model.predict(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "first-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conservative-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize submission\n",
    "\n",
    "f = open('submission.txt','w+')\n",
    "for y in y_pred:\n",
    "    f.write(str(int(y[0])) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
